{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Nval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b610625fa8c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mNkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Neighbourhood'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Neighbourhood'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mGkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mNval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mGval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mNmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Nval' is not defined"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "#mapping data\n",
    "\n",
    "Nkey = train['Neighbourhood'].append(test['Neighbourhood']).dropna().unique()\n",
    "Gkey = train['Gender'].append(test['Gender']).dropna().unique()\n",
    "Nval = np.arange(1,len(Nval)+1)\n",
    "Gval = np.arange(1,len(Gval)+1)\n",
    "Nmap = dict((key, value) for (key, value) in zip(Nkey,Nval))\n",
    "Gmap = dict((key, value) for (key, value) in zip(Gkey,Gval))\n",
    "\n",
    "train['Gender'].replace(Gmap, inplace=True)\n",
    "train['Neighbourhood'].replace(Nmap, inplace=True)\n",
    "test['Gender'].replace(Gmap, inplace=True)\n",
    "test['Neighbourhood'].replace(Nmap, inplace=True)\n",
    "#set the type of variables\n",
    "cat_var = ['Gender','Neighbourhood','Scholarship', 'Hipertension',\n",
    "           'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received', 'No-show']\n",
    "time_var = ['ScheduledDay','AppointmentDay']\n",
    "#drop the time of ScheduledDay variables, since AppointmentDay does not have time information\n",
    "train['ScheduledDay']=train['ScheduledDay'].astype('str').map(lambda x: x[0:10])\n",
    "test['ScheduledDay']=test['ScheduledDay'].astype('str').map(lambda x: x[0:10])\n",
    "#change the variable type\n",
    "for i in time_var:\n",
    "    train[i]=pd.to_datetime(train[i])\n",
    "    test[i]=pd.to_datetime(test[i])\n",
    "for i in cat_var:\n",
    "    train[i] = train[i].astype('category')\n",
    "    test[i] = test[i].astype('category')\n",
    "#make new column day_diff    \n",
    "train['day_diff'] = (train['AppointmentDay'] - train['ScheduledDay']).map(lambda x : x.days)\n",
    "train = train.iloc[ :, 3:]\n",
    "\n",
    "test['day_diff'] = (test['AppointmentDay'] - test['ScheduledDay']).map(lambda x : x.days)\n",
    "test = test.iloc[ :, 3:]\n",
    "#remove the ScheduledDay AppointmentDay columns\n",
    "train = train.drop(columns=['ScheduledDay','AppointmentDay'])\n",
    "test = test.drop(columns=['ScheduledDay','AppointmentDay'])\n",
    "if 0:\n",
    "    train.drop(columns='Neighbourhood')\n",
    "    test.drop(columns='Neighbourhood')\n",
    "#imputation mode with categorical variables, median with numerical variables\n",
    "Imputation = 0\n",
    "if Imputation:\n",
    "    for i in cat_var:\n",
    "        train[i].fillna(train[i].mode()[0], inplace=True)    \n",
    "        test[i].fillna(test[i].mode()[0], inplace=True)  \n",
    "    for i in train.columns:\n",
    "        if i not in cat_var:\n",
    "            train[i].fillna(train[i].median(), inplace=True)\n",
    "            test[i].fillna(test[i].median(), inplace=True)\n",
    "if not Imputation:\n",
    "    for i in cat_var:\n",
    "        try:\n",
    "            test[i].cat.add_categories(0)\n",
    "        finally:\n",
    "            test[i].fillna(0,inplace=True)\n",
    "        try:\n",
    "            train[i].cat.add_categories(0)\n",
    "        finally:\n",
    "            train[i].fillna(0,inplace=True)\n",
    "        #finally:\n",
    "\n",
    "#set the X,Y variables\n",
    "y = train['No-show']\n",
    "X = train.drop(columns='No-show')\n",
    "#train test split for CV\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "#test data\n",
    "test_X = test.drop(columns='No-show')\n",
    "test_y = test['No-show']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e610524f3edf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#oversampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_samp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_samp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_samp\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_samp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_samp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_samp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_samp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#oversampling\n",
    "X_samp, y_samp = SMOTE(random_state=0).fit_sample(X_train, y_train)\n",
    "X_samp= pd.DataFrame(X_samp)\n",
    "X_samp.columns = X_train.columns\n",
    "X_train, y_train = X_samp, y_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=300, max_depth=10,random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "def rf_with_cutoff(test_X=test_X,test_y=test_y,clf=clf,silence=True):\n",
    "    tmp = [x[0] for x in clf.predict_proba(test_X)]\n",
    "    cutoff = sorted(tmp)[int((train['No-show'].sum()/train.shape[0]) * len(clf.predict(test_X)))]\n",
    "    sub = pd.Series([x[0] < cutoff for x in clf.predict_proba(test_X)]).map(int)\n",
    "    print('f1 score : ' + str(f1_score(y_pred=sub,y_true=test_y)))\n",
    "    if not silence:\n",
    "        return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38455529516794995"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_valid),y_true=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3759567762269248"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(test_X),y_true=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.299609\tvalidation_1-error:0.342236\tvalidation_0-f1_err:0.299164\tvalidation_1-f1_err:0.634018\n",
      "Multiple eval metrics have been passed: 'validation_1-f1_err' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-f1_err hasn't improved in 200 rounds.\n",
      "[10]\tvalidation_0-error:0.263321\tvalidation_1-error:0.354123\tvalidation_0-f1_err:0.242039\tvalidation_1-f1_err:0.585912\n",
      "[20]\tvalidation_0-error:0.267669\tvalidation_1-error:0.368298\tvalidation_0-f1_err:0.243762\tvalidation_1-f1_err:0.594922\n",
      "[30]\tvalidation_0-error:0.267669\tvalidation_1-error:0.368298\tvalidation_0-f1_err:0.243762\tvalidation_1-f1_err:0.594922\n",
      "[40]\tvalidation_0-error:0.265651\tvalidation_1-error:0.366706\tvalidation_0-f1_err:0.242029\tvalidation_1-f1_err:0.595029\n",
      "[50]\tvalidation_0-error:0.255944\tvalidation_1-error:0.339451\tvalidation_0-f1_err:0.240368\tvalidation_1-f1_err:0.590653\n",
      "[60]\tvalidation_0-error:0.255804\tvalidation_1-error:0.339501\tvalidation_0-f1_err:0.240208\tvalidation_1-f1_err:0.590791\n",
      "[70]\tvalidation_0-error:0.255804\tvalidation_1-error:0.339501\tvalidation_0-f1_err:0.240208\tvalidation_1-f1_err:0.590791\n",
      "[80]\tvalidation_0-error:0.246564\tvalidation_1-error:0.315677\tvalidation_0-f1_err:0.238135\tvalidation_1-f1_err:0.591574\n",
      "[90]\tvalidation_0-error:0.245061\tvalidation_1-error:0.315677\tvalidation_0-f1_err:0.236339\tvalidation_1-f1_err:0.591574\n",
      "[100]\tvalidation_0-error:0.245045\tvalidation_1-error:0.315677\tvalidation_0-f1_err:0.236321\tvalidation_1-f1_err:0.591574\n",
      "[110]\tvalidation_0-error:0.243417\tvalidation_1-error:0.311201\tvalidation_0-f1_err:0.236265\tvalidation_1-f1_err:0.593812\n",
      "[120]\tvalidation_0-error:0.243394\tvalidation_1-error:0.311201\tvalidation_0-f1_err:0.236237\tvalidation_1-f1_err:0.593812\n",
      "[130]\tvalidation_0-error:0.240114\tvalidation_1-error:0.306625\tvalidation_0-f1_err:0.234292\tvalidation_1-f1_err:0.59479\n",
      "[140]\tvalidation_0-error:0.239834\tvalidation_1-error:0.306476\tvalidation_0-f1_err:0.23399\tvalidation_1-f1_err:0.594673\n",
      "[150]\tvalidation_0-error:0.235331\tvalidation_1-error:0.300657\tvalidation_0-f1_err:0.231713\tvalidation_1-f1_err:0.59893\n",
      "[160]\tvalidation_0-error:0.235346\tvalidation_1-error:0.301353\tvalidation_0-f1_err:0.231636\tvalidation_1-f1_err:0.598893\n",
      "[170]\tvalidation_0-error:0.234536\tvalidation_1-error:0.305232\tvalidation_0-f1_err:0.228245\tvalidation_1-f1_err:0.586431\n",
      "[180]\tvalidation_0-error:0.234334\tvalidation_1-error:0.303641\tvalidation_0-f1_err:0.228759\tvalidation_1-f1_err:0.587867\n",
      "[190]\tvalidation_0-error:0.234201\tvalidation_1-error:0.304039\tvalidation_0-f1_err:0.228673\tvalidation_1-f1_err:0.588071\n",
      "[200]\tvalidation_0-error:0.234139\tvalidation_1-error:0.303989\tvalidation_0-f1_err:0.228442\tvalidation_1-f1_err:0.587353\n",
      "Stopping. Best iteration:\n",
      "[9]\tvalidation_0-error:0.279627\tvalidation_1-error:0.380235\tvalidation_0-f1_err:0.250171\tvalidation_1-f1_err:0.582387\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.9, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=4, min_child_weight=3, missing=None, n_estimators=3000,\n",
       "       n_jobs=1, nthread=8, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = 1-f1_score(y_true, np.round(y_pred))\n",
    "    return 'f1_err', err\n",
    "\n",
    "alg = XGBClassifier(learning_rate=0.01, n_estimators=3000, max_depth=4,\n",
    "                    min_child_weight=3, colsample_bytree=0.9,\n",
    "                    objective='binary:logistic', nthread=8, seed=27, random_state = 0)\n",
    "\n",
    "alg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train),(X_valid, y_valid)], #모델에서 자체적으로 평가에 사용할 데이터\n",
    "        eval_metric=f1_eval, #모델의 목적함수 지정(최소화할 목적함수 1-f1_score)\n",
    "        early_stopping_rounds=200, #1o0 Interations 동안 최대화 되지 않으면 stop\n",
    "        verbose=10) #Iteration 과정을 10 단위로 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_with_cutoff(test_X=test_X,test_y=test_y,alg=alg,silence = True):\n",
    "    tmp = [x[0] for x in alg.predict_proba(test_X)]\n",
    "    cutoff = sorted(tmp)[int((train['No-show'].sum()/train.shape[0]) * len(test_y))]\n",
    "    sub = pd.Series([x[0] < cutoff for x in alg.predict_proba(test_X)]).map(int)\n",
    "    print('f1 score : ' + str(f1_score(y_pred=sub,y_true=test_y)))\n",
    "    if not silence:\n",
    "        return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4176125542774435"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=alg.predict(X_valid),y_true=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41051505425645735"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=alg.predict(test_X),y_true=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
