{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 190323 SparkSQL\n",
    "\n",
    "thanks to 12기 지훈님,,,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reference : https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-28-91.us-west-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ 코드 작성 절차(Word Count) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 세션 생성\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"sample\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|A bad penny alway...|\n",
      "|A barking dog nev...|\n",
      "|A bird in the han...|\n",
      "|A cat may look at...|\n",
      "|A chain is only a...|\n",
      "|A change is as go...|\n",
      "|A dog is a man's ...|\n",
      "|A drowning man wi...|\n",
      "|A fish always rot...|\n",
      "|A fool and his mo...|\n",
      "|A friend in need ...|\n",
      "|A golden key can ...|\n",
      "|A good beginning ...|\n",
      "|A good man is har...|\n",
      "|A house divided a...|\n",
      "|A person is known...|\n",
      "|A house is not a ...|\n",
      "|A journey of a th...|\n",
      "|A leopard cannot ...|\n",
      "|A little knowledg...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 파일 불러오기\n",
    "source = \"/FileStore/tables/countMe.txt\" # 각자 파일이 있는 경로로 재지정 해주세요!         \n",
    "df = spark.read.text(source)  \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"value\" 컬럼의 데이터를 띄어쓰기 단위로 split하여 분리된 문자열 배열을 포함한 칼럼을 하나의 값을 가진 여러 개의 열로 확장\n",
    "# 그리고 새로 만들어진 컬럼의 별칭을 \"word\"라고 한다\n",
    "wordDF= df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "|      A|\n",
      "|    bad|\n",
      "|  penny|\n",
      "| always|\n",
      "|  turns|\n",
      "|     up|\n",
      "|      A|\n",
      "|barking|\n",
      "|    dog|\n",
      "|  never|\n",
      "|  bites|\n",
      "|      A|\n",
      "|   bird|\n",
      "|     in|\n",
      "|    the|\n",
      "|   hand|\n",
      "|     is|\n",
      "|  worth|\n",
      "|    two|\n",
      "|     in|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"word\" 컬럼을 기준으로 groupBy하여 각 단어가 몇 번씩 나왔는지를 count한다\n",
    "result=wordDF.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|      July|    1|\n",
      "|     those|    5|\n",
      "|     spoil|    3|\n",
      "|    travel|    2|\n",
      "|       few|    1|\n",
      "|pack-drill|    1|\n",
      "|    waters|    1|\n",
      "|    harder|    1|\n",
      "|      hope|    1|\n",
      "|      some|    1|\n",
      "|    taking|    1|\n",
      "|   Sabbath|    1|\n",
      "|     parts|    1|\n",
      "|      lies|    1|\n",
      "|    Mighty|    1|\n",
      "|  Tomorrow|    2|\n",
      "|   vinegar|    1|\n",
      "|   stomach|    2|\n",
      "|   showers|    2|\n",
      "|   flowers|    2|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "outdir = \"file:///home/ubuntu/19-1Engineering/Practice/output/\" # 각자 파일이 있는 경로로 재지정 해주세요!\n",
    "result.write.format(\"csv\").save(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 스파크 컨텍스트 종료\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame을 생성 후 SQL의 SELECT 문으로 원하는 데이터를 조회해보자\n",
    "# 위에 코드를 하나로 합쳐놓았습니다\n",
    "# 안돌리셔도 돼요!\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"sample\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "source = \"/FileStore/tables/countMe.txt\"         \n",
    "df = spark.read.text(source)  \n",
    "\n",
    "wordDF = df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "result = wordDF.groupBy(\"word\").count()\n",
    "result.show()\n",
    "\n",
    "outdir = \"file:///home/ubuntu/19-1Engineering/Practice/output/\"\n",
    "result.write.format(\"csv\").save(outdir)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ DataFrame 생성 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"sample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 외부 데이터 소스로부터 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n",
      "+-----------+\n",
      "|      value|\n",
      "+-----------+\n",
      "|Michael, 29|\n",
      "|   Andy, 30|\n",
      "| Justin, 19|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commonDir = \"/FileStore/tables/\"\n",
    "df1 = spark.read.json(commonDir + \"people.json\")\n",
    "df2 = spark.read.parquet(commonDir + \"users.parquet\")\n",
    "df3 = spark.read.text(commonDir + \"people.txt\")\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD로부터 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------+\n",
      "|age|           job|  name|\n",
      "+---+--------------+------+\n",
      "|  7|       student|hayoon|\n",
      "| 13|       student|sunwoo|\n",
      "|  5|kindergartener| hajoo|\n",
      "| 13|       student|jinwoo|\n",
      "+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row1 = Row(name=\"hayoon\", age=7, job=\"student\")\n",
    "row2 = Row(name=\"sunwoo\", age=13, job=\"student\")\n",
    "row3 = Row(name=\"hajoo\", age=5, job=\"kindergartener\")\n",
    "row4 = Row(name=\"jinwoo\", age=13, job=\"student\")\n",
    "\n",
    "data = [row1, row2, row3, row4]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "df4 = spark.createDataFrame(data)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List로부터 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------+\n",
      "|age|           job|  name|\n",
      "+---+--------------+------+\n",
      "|  7|       student|hayoon|\n",
      "| 13|       student|sunwoo|\n",
      "|  5|kindergartener| hajoo|\n",
      "| 13|       student|jinwoo|\n",
      "+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row1 = Row(name=\"hayoon\", age=7, job=\"student\")\n",
    "row2 = Row(name=\"sunwoo\", age=13, job=\"student\")\n",
    "row3 = Row(name=\"hajoo\", age=5, job=\"kindergartener\")\n",
    "row4 = Row(name=\"jinwoo\", age=13, job=\"student\")\n",
    "\n",
    "data = [row1, row2, row3, row4]\n",
    "\n",
    "df5 = spark.createDataFrame(data)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store2|   note|    20| 2000|\n",
      "|store2|    bag|    10| 5000|\n",
      "|store1|   note|    15| 1000|\n",
      "|store1|    pen|    20| 5000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = (\"store2\", \"note\", 20, 2000)\n",
    "d2 = (\"store2\", \"bag\", 10, 5000)\n",
    "d3 = (\"store1\", \"note\", 15, 1000)\n",
    "d4 = (\"store1\", \"pen\", 20, 5000)\n",
    "df8 = spark.createDataFrame([d1, d2, d3, d4]).toDF(\"store\", \"product\", \"amount\", \"price\")\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema 지정으로 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------+\n",
      "|  name|age|           job|\n",
      "+------+---+--------------+\n",
      "|hayoon|  7|       student|\n",
      "|sunwoo| 13|       student|\n",
      "| hajoo|  5|kindergartener|\n",
      "|jinwoo| 13|       student|\n",
      "+------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf1 = StructField(\"name\", StringType(), True)\n",
    "sf2 = StructField(\"age\", IntegerType(), True)\n",
    "sf3 = StructField(\"job\", StringType(), True)\n",
    "\n",
    "schema = StructType([sf1, sf2, sf3])\n",
    "\n",
    "r1 = Row(name=\"hayoon\", age=7, job=\"student\")\n",
    "r2 = Row(name=\"sunwoo\", age=13, job=\"student\")\n",
    "r3 = Row(name=\"hajoo\", age=5, job=\"kindergartener\")\n",
    "r4 = Row(name=\"jinwoo\", age=13, job=\"student\")\n",
    "rows = [r1, r2, r3, r4]\n",
    "\n",
    "df6 = spark.createDataFrame(rows, schema)\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class를 이용하여 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|count|word|\n",
      "+-----+----+\n",
      "|    1|  w1|\n",
      "|    1|  w2|\n",
      "+-----+----+\n",
      "\n",
      "+-----+----+\n",
      "|count|word|\n",
      "+-----+----+\n",
      "|    1|  w1|\n",
      "|    1|  w3|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#사용자가 class를 정의한 후, 새로운 객체를 생성하고 그 객체를 이용해서 Dataframe으로 생성\n",
    "class Word:\n",
    "    def __init__(self, word, count):\n",
    "        self.word = word\n",
    "        self.count = count\n",
    "\n",
    "ldf = spark.createDataFrame([Word(\"w1\", 1), Word(\"w2\", 1)])\n",
    "rdf = spark.createDataFrame([Word(\"w1\", 1), Word(\"w3\", 1)])\n",
    "ldf.show()\n",
    "rdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ 주요 연산 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 액션 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------+\n",
      "|  name|age|           job|\n",
      "+------+---+--------------+\n",
      "|hayoon|  7|       student|\n",
      "|sunwoo| 13|       student|\n",
      "| hajoo|  5|kindergartener|\n",
      "|jinwoo| 13|       student|\n",
      "+------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show()\n",
    "# DataFrame에 저장된 데이터를 화면에 출력\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head() / first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='hayoon', age=7, job='student')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head, first\n",
    "# 첫 번째 로우를 리턴\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='hayoon', age=7, job='student'),\n",
       " Row(name='sunwoo', age=13, job='student'),\n",
       " Row(name='hajoo', age=5, job='kindergartener')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take(n)\n",
    "# 첫 n개를 보여줌\n",
    "df6.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "# RDD의 count()연산과 동일, 로우의 개수를 리턴\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='hayoon', age=7, job='student'),\n",
       " Row(name='sunwoo', age=13, job='student'),\n",
       " Row(name='hajoo', age=5, job='kindergartener'),\n",
       " Row(name='jinwoo', age=13, job='student')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect\n",
    "# 데이터셋에 있는 데이터를 로컬 컬렉션(배열) 형태로 리턴\n",
    "# 메모리 부족 에러가 발생하지 않도록 주의!\n",
    "df6.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              age|\n",
      "+-------+-----------------+\n",
      "|  count|                4|\n",
      "|   mean|              9.5|\n",
      "| stddev|4.123105625617661|\n",
      "|    min|                5|\n",
      "|    max|               13|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "df6.describe(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|summary|           job|\n",
      "+-------+--------------+\n",
      "|  count|             4|\n",
      "|   mean|          null|\n",
      "| stddev|          null|\n",
      "|    min|kindergartener|\n",
      "|    max|       student|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.describe(\"job\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. 기본 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "persist() / cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, job: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache는 persist와 동일한 기능 제공\n",
    "# 작업중인 데이터를 메모리에 저장\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "df6.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://스파크셸실행시킨서버:4040 으로 접속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://스파크셸실행시킨서버:4040 다시 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema(), columns, dtypes, schema\n",
    "# 스키마 정보를 조회하는 메서드와 리스트들\n",
    "\n",
    "df6.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'job']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메서드가 아닙니다. 리스트를 반환합니다.\n",
    "df6.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int'), ('job', 'string')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메서드가 아닙니다. 리스트를 반환합니다.\n",
    "df6.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(age,LongType,true),StructField(job,StringType,true)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메서드가 아닙니다. 리스트를 반환합니다.\n",
    "df7.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createOrReplaceTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|sunwoo| 13|\n",
      "|jinwoo| 13|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# createOrReplaceTempView()\n",
    "\n",
    "# DataFrame을 SQL문을 이용해 테이블처럼 조회할 수 있도록 등록해줍니다.\n",
    "# 단, 이 메서드로 등록된 테이블은 스파크세션이 유지되는 동안만 유효합니다.\n",
    "# (세션이 종료되면 사라짐)\n",
    "\n",
    "df6.createOrReplaceTempView(\"Example\")\n",
    "spark.sql(\"SELECT name, age FROM Example WHERE age > 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(age#100) && (age#100 > 10))\n",
      "+- InMemoryTableScan [name#99, age#100], [isnotnull(age#100), (age#100 > 10)]\n",
      "      +- InMemoryRelation [name#99, age#100, job#101], true, 10000, StorageLevel(disk, memory, 2 replicas)\n",
      "            +- Scan ExistingRDD[name#99,age#100,job#101]\n"
     ]
    }
   ],
   "source": [
    "# explain()\n",
    "\n",
    "# 데이터프레임 처리와 관련된 실행 계획 정보를 출력합니다.\n",
    "\n",
    "spark.sql(\"SELECT name, age FROM Example WHERE age > 10\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비타입 트랜스포메이션\n",
    "\n",
    "데이터의 실제 타입을 사용하지 않는 변환 연산을 수행한다는 의미에서 붙여진 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+\n",
      "|  name|age|    job|\n",
      "+------+---+-------+\n",
      "|sunwoo| 13|student|\n",
      "|jinwoo| 13|student|\n",
      "+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL문을 직접 이용한 연산\n",
    "\n",
    "spark.sql(\"SELECT name, age, job FROM Example WHERE age > 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+\n",
      "|  name|age|    job|\n",
      "+------+---+-------+\n",
      "|sunwoo| 13|student|\n",
      "|jinwoo| 13|student|\n",
      "+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이 연산과 동일합니다.\n",
    "# 이런 형태의 연산을 비타입 트랜스포메이션 연산이라 합니다.\n",
    "\n",
    "df6.where(df6['age'] > 10).show()\n",
    "#df6.where(df6.age > 20).show() # 이런 형태도 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias(), as()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|(age + 1)|\n",
      "+---------+\n",
      "|        8|\n",
      "|       14|\n",
      "|        6|\n",
      "|       14|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(df6['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  8|\n",
      "| 14|\n",
      "|  6|\n",
      "| 14|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias()\n",
    "\n",
    "# 컬럼 명에 원하는 이름을 붙여줄 수 있습니다. \n",
    "df6.select((df6['age'] + 1).alias(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# isIn()\n",
    "\n",
    "# 컬럼의 값이 인자로 지정된 값에 포함되어 있는지의 여부를 확인합니다.\n",
    "# 아래의 예제는 유효한 값을 broadcast 변수에 담아 필터처럼 사용합니다.\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "nums = spark.sparkContext.broadcast([1, 3, 5, 7, 9])\n",
    "rdd = spark.sparkContext.parallelize(range(0, 7)).map(lambda v: Row(v))\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.where(df._1.isin(nums.value)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|type|\n",
      "+---+----+\n",
      "|  0|even|\n",
      "|  1| odd|\n",
      "|  2|even|\n",
      "|  3| odd|\n",
      "|  4|even|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when()\n",
    "\n",
    "from pyspark.sql import functions\n",
    "\n",
    "ds = spark.range(0, 5)\n",
    "col = functions.when(ds.id % 2 == 0, \"even\").otherwise(\"odd\").alias(\"type\")\n",
    "ds.select(ds.id, col).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(store='store1', min(amount)=15), Row(store='store2', min(amount)=10)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = (\"store2\", \"note\", 20, 2000)\n",
    "d2 = (\"store2\", \"bag\", 10, 5000)\n",
    "d3 = (\"store1\", \"note\", 15, 1000)\n",
    "d4 = (\"store1\", \"pen\", 20, 5000)\n",
    "df = spark.createDataFrame([d1, d2, d3, d4]).toDF(\"store\", \"product\", \"amount\", \"price\")\n",
    "\n",
    "gdf = df.groupBy(df.store)\n",
    "sorted(gdf.agg({\"*\": \"count\"}).collect())\n",
    "\n",
    "sorted(gdf.agg(functions.min(df.amount)).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max(), mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      13|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(max(df6['age'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(job)|\n",
      "+--------+\n",
      "| student|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(max(df6['job'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|     9.5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(mean(df6['age'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(job)|\n",
      "+--------+\n",
      "|    null|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(mean(df6['job'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect_list() , collect_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------+\n",
      "|  name|age|           job|\n",
      "+------+---+--------------+\n",
      "|hayoon|  7|       student|\n",
      "|sunwoo| 13|       student|\n",
      "| hajoo|  5|kindergartener|\n",
      "|jinwoo| 13|       student|\n",
      "|hayoon|  7|       student|\n",
      "|sunwoo| 13|       student|\n",
      "| hajoo|  5|kindergartener|\n",
      "|jinwoo| 13|       student|\n",
      "+------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 특정 컬럼 값을 모아서 하나의 리스트 또는 세트(set)로 된 컬럼을 생성함\n",
    "# 리스트는 중복 포함, 세트는 중복 미포함\n",
    "\n",
    "doubleDf = df6.union(df6)\n",
    "doubleDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(collect_list(name)=['hayoon', 'sunwoo', 'hajoo', 'jinwoo', 'hayoon', 'sunwoo', 'hajoo', 'jinwoo'])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleDf.select(collect_list('name')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(collect_set(name)=['hajoo', 'sunwoo', 'jinwoo', 'hayoon'])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleDf.select(collect_set('name')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|collect_list(name)[0]|\n",
      "+---------------------+\n",
      "|               hayoon|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doubleDf.select(collect_list(\"name\")[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|collect_set(name)[4]|\n",
      "+--------------------+\n",
      "|                null|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doubleDf.select(collect_set(\"name\")[4]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count() / countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|count(name)|count(DISTINCT name)|\n",
      "+-----------+--------------------+\n",
      "|          8|                   4|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doubleDf.select(count(\"name\"), countDistinct(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store2|   note|    20| 2000|\n",
      "|store2|    bag|    10| 5000|\n",
      "|store1|   note|    15| 1000|\n",
      "|store1|    pen|    20| 5000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(price)|\n",
      "+----------+\n",
      "|     13000|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(sum(\"price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|(sum(price) / count(price))|\n",
      "+---------------------------+\n",
      "|                     3250.0|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(sum(\"price\") / count(\"price\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "round() / sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|SQRT((sum(price) / count(price)))|\n",
      "+---------------------------------+\n",
      "|                 57.0087712549569|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(sqrt(sum(\"price\") / count(\"price\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|round(SQRT((sum(price) / count(price))), 3)|\n",
      "+-------------------------------------------+\n",
      "|                                     57.009|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(round(sqrt(sum(\"price\") / count(\"price\")),3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desc() / asc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store2|    bag|    10| 5000|\n",
      "|store1|    pen|    20| 5000|\n",
      "|store2|   note|    20| 2000|\n",
      "|store1|   note|    15| 1000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.sort(desc(\"price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store1|    pen|    20| 5000|\n",
      "|store2|    bag|    10| 5000|\n",
      "|store2|   note|    20| 2000|\n",
      "|store1|   note|    15| 1000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.sort(desc(\"price\"),asc(\"store\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|max(amount)|min(price)|\n",
      "+-----------+----------+\n",
      "|         20|      1000|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.agg(max(\"amount\"), min(\"price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|max(amount)|min(price)|\n",
      "+-----------+----------+\n",
      "|         20|      1000|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(max(\"amount\"), min(\"price\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "| store|sum(price)|\n",
      "+------+----------+\n",
      "|store2|      7000|\n",
      "|store1|      6000|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.groupBy(\"store\").sum(\"price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "| store|product|sum(price)|\n",
      "+------+-------+----------+\n",
      "|store1|   note|      1000|\n",
      "|store2|    bag|      5000|\n",
      "|store1|    pen|      5000|\n",
      "|store2|   note|      2000|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.groupBy(\"store\", \"product\").sum(\"price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "| store|product|sum(price)|\n",
      "+------+-------+----------+\n",
      "|  null|   null|     13000|\n",
      "|  null|    bag|      5000|\n",
      "|  null|   note|      3000|\n",
      "|  null|    pen|      5000|\n",
      "|store1|   null|      6000|\n",
      "|store1|   note|      1000|\n",
      "|store1|    pen|      5000|\n",
      "|store2|   null|      7000|\n",
      "|store2|    bag|      5000|\n",
      "|store2|   note|      2000|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.cube(\"store\", \"product\").sum(\"price\")\\\n",
    ".sort(asc(\"store\"), asc(\"product\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store2|   note|    20| 2000|\n",
      "|store2|    bag|    10| 5000|\n",
      "|store1|   note|    15| 1000|\n",
      "|store1|    pen|    20| 5000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select([c for c in df8.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+) 추가!\n",
    "\n",
    "withColumn() : 기존 데이터프레임에서 새로운 컬럼을 추가할 때 쓰는 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 10|Alice|\n",
      "|  5|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r1 = Row(name=\"Alice\", age=10)\n",
    "r2 = Row(name=\"Bob\", age=5)\n",
    "\n",
    "df = sc.parallelize([r1, r2]).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "|age| name| job|\n",
      "+---+-----+----+\n",
      "| 10|Alice|null|\n",
      "|  5|  Bob|null|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.withColumn('job', lit(None).cast(StringType()))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "na.fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "|age| name|    job|\n",
      "+---+-----+-------+\n",
      "| 10|Alice|student|\n",
      "|  5|  Bob|student|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.na.fill(\"student\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "|age|name|job|\n",
      "+---+----+---+\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "|age|name|job|\n",
      "+---+----+---+\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.na.drop(subset = 'job').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ Pandas 연동 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/ubuntu/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.14 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from pyarrow)\n",
      "Requirement already satisfied: six>=1.0.0 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from pyarrow)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amount': [20, 10, 15, 20],\n",
       " 'name': ['store2', 'store2', 'store1', 'store1'],\n",
       " 'price': [2000, 5000, 1000, 5000],\n",
       " 'product': ['note', 'bag', 'note', 'pen']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "sales_data = {'name' : ['store2', 'store2', 'store1', 'store1'],\n",
    "             'product' : ['note', 'bag', 'note', 'pen'],\n",
    "             'amount' : [20, 10, 15, 20],\n",
    "             'price' : [2000, 5000, 1000, 5000]}\n",
    "sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>store2</td>\n",
       "      <td>2000</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>store2</td>\n",
       "      <td>5000</td>\n",
       "      <td>bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>store1</td>\n",
       "      <td>1000</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>store1</td>\n",
       "      <td>5000</td>\n",
       "      <td>pen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount    name  price product\n",
       "0      20  store2   2000    note\n",
       "1      10  store2   5000     bag\n",
       "2      15  store1   1000    note\n",
       "3      20  store1   5000     pen"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pd.DataFrame(sales_data)\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Dataframe -> pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+-------+\n",
      "|amount|  name|price|product|\n",
      "+------+------+-----+-------+\n",
      "|    20|store2| 2000|   note|\n",
      "|    10|store2| 5000|    bag|\n",
      "|    15|store1| 1000|   note|\n",
      "|    20|store1| 5000|    pen|\n",
      "+------+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  name|count|\n",
      "+------+-----+\n",
      "|store2|    2|\n",
      "|store1|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df =spark.createDataFrame(pdf).groupBy(\"name\").count()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " pyspark DataFrame -> Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>store2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  count\n",
       "0  store2      2\n",
       "1  store1      2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf2 = df.toPandas()\n",
    "pdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas UDF(User Defined Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+-------+\n",
      "|amount|  name|price|product|\n",
      "+------+------+-----+-------+\n",
      "|    20|store2| 2000|   note|\n",
      "|    10|store2| 5000|    bag|\n",
      "|    15|store1| 1000|   note|\n",
      "|    20|store1| 5000|    pen|\n",
      "+------+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totalDf = spark.createDataFrame(pdf)\n",
    "totalDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+-------+-----------+\n",
      "|amount|  name|price|product|total_price|\n",
      "+------+------+-----+-------+-----------+\n",
      "|    20|store2| 2000|   note|      40000|\n",
      "|    10|store2| 5000|    bag|      50000|\n",
      "|    15|store1| 1000|   note|      15000|\n",
      "|    20|store1| 5000|    pen|     100000|\n",
      "+------+------+-----+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_total_price(amount, price):\n",
    "    return amount * price\n",
    "\n",
    "total_price = pandas_udf(get_total_price, returnType=LongType())\n",
    "totalDf.withColumn(\"total_price\", total_price(totalDf[\"amount\"], totalDf[\"price\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark에서도 UDF를 사용한다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "# 1\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "\n",
    "# 2\n",
    "def to_upper(s):\n",
    "     if s is not None:\n",
    "        return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|slen(name)|    NAME|\n",
      "+----------+--------+\n",
      "|         8|John Doe|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
    "df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5.5.2.1.1절 ~ 5.5.2.2.4절\n",
    "def runBasicOpsEx(spark, sc, df):\n",
    "    df.show()\n",
    "    df.head()\n",
    "    df.first()\n",
    "    df.take(2)\n",
    "    df.count()\n",
    "    df.collect()\n",
    "    df.describe(\"age\").show()\n",
    "    df.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
    "    df.printSchema()\n",
    "    df.columns\n",
    "    df.dtypes\n",
    "    df.schema\n",
    "    df.createOrReplaceTempView(\"users\")\n",
    "    spark.sql(\"select name, age from users where age > 20\").show()\n",
    "    spark.sql(\"select name, age from users where age > 20\").explain()\n",
    "\n",
    "\n",
    "# 5.5.2.4절\n",
    "def runColumnEx(spark, sc, df):\n",
    "    df.where(df.age > 10).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.2절\n",
    "def runAlias(spark, sc, df):\n",
    "    df.select(df.age + 1).show()\n",
    "    df.select((df.age + 1).alias(\"age\")).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.3절\n",
    "def runIsinEx(spark, sc):\n",
    "    nums = spark.sparkContext.broadcast([1, 3, 5, 7, 9])\n",
    "    rdd = spark.sparkContext.parallelize(range(0, 10)).map(lambda v: Row(v))\n",
    "    df = spark.createDataFrame(rdd)\n",
    "    df.where(df._1.isin(nums.value)).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.4절\n",
    "def runWhenEx(spark, sc):\n",
    "    ds = spark.range(0, 5)\n",
    "    col = when(ds.id % 2 == 0, \"even\").otherwise(\"odd\").alias(\"type\")\n",
    "    ds.select(ds.id, col).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.5절\n",
    "def runMaxMin(spark, df):\n",
    "    min_col = min(\"age\")\n",
    "    max_col = max(\"age\")\n",
    "    df.select(min_col, max_col).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.6절 ~ 5.5.2.4.9절\n",
    "def runAggregateFunctions(spark, df1, df2):\n",
    "    # collect_list, collect_set\n",
    "    doubledDf1 = df1.union(df1)\n",
    "    doubledDf1.select(functions.collect_list(doubledDf1[\"name\"])).show(truncate=False)\n",
    "    doubledDf1.select(functions.collect_set(doubledDf1[\"name\"])).show(truncate=False)\n",
    "\n",
    "    # count, countDistinct\n",
    "    doubledDf1.select(functions.count(doubledDf1[\"name\"]), functions.countDistinct(doubledDf1[\"name\"])).show(\n",
    "        truncate=False)\n",
    "\n",
    "    # sum\n",
    "    df2.printSchema()\n",
    "    df2.select(sum(df2[\"price\"])).show(truncate=False)\n",
    "\n",
    "    # grouping, grouping_id\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping(df2[\"store\"])).show(truncate=False)\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping_id(df2[\"store\"], df2[\"product\"])).show(\n",
    "        truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.10 ~ 5.5.2.4.11 절\n",
    "def runCollectionFunctions(spark):\n",
    "    df = spark.createDataFrame([{'numbers': '9,1,5,3,9'}])\n",
    "    arrayCol = split(df.numbers, \",\")\n",
    "\n",
    "    # array_contains, size\n",
    "    df.select(arrayCol, array_contains(arrayCol, 2), size(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # sort_array()\n",
    "    df.select(arrayCol, sort_array(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # explode, posexplode\n",
    "    df.select(explode(arrayCol)).show(truncate=False)\n",
    "    df.select(posexplode(arrayCol)).show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.10 ~ 5.5.2.4.11 절\n",
    "def runCollectionFunctions(spark):\n",
    "    df = spark.createDataFrame([{'numbers': '9,1,5,3,9'}])\n",
    "    arrayCol = split(df.numbers, \",\")\n",
    "\n",
    "    # array_contains, size\n",
    "    df.select(arrayCol, array_contains(arrayCol, 2), size(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # sort_array()\n",
    "    df.select(arrayCol, sort_array(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # explode, posexplode\n",
    "    df.select(explode(arrayCol)).show(truncate=False)\n",
    "    df.select(posexplode(arrayCol)).show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.12 ~ 5.5.2.4.14절\n",
    "def runDateFunctions(spark):\n",
    "    f1 = StructField(\"d1\", StringType(), True)\n",
    "    f2 = StructField(\"d2\", StringType(), True)\n",
    "    schema1 = StructType([f1, f2])\n",
    "\n",
    "    df = spark.createDataFrame([(\"2017-12-25 12:00:05\", \"2017-12-25\")], schema1)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # current_date, unix_timestamp, to_date\n",
    "    d3 = current_date().alias(\"d3\")\n",
    "    d4 = unix_timestamp(df[\"d1\"].alias(\"d4\"))\n",
    "    d5 = to_date(df[\"d2\"].alias(\"d5\"))\n",
    "    d6 = to_date(d4.cast(\"timestamp\")).alias(\"d6\")\n",
    "    df.select(df[\"d1\"], df[\"d2\"], d3, d4, d5, d6).show(truncate=False)\n",
    "\n",
    "    # add_months, date_add, last_day\n",
    "    d7 = add_months(d6, 2).alias(\"d7\")\n",
    "    d8 = date_add(d6, 2).alias(\"d8\")\n",
    "    d9 = last_day(d6).alias(\"d9\")\n",
    "    df.select(df[\"d1\"], df[\"d2\"], d7, d8, d9).show(truncate=False)\n",
    "\n",
    "    # window\n",
    "    f3 = StructField(\"date\", StringType(), True)\n",
    "    f4 = StructField(\"product\", StringType(), True)\n",
    "    f5 = StructField(\"amount\", IntegerType(), True)\n",
    "    schema2 = StructType([f3, f4, f5])\n",
    "\n",
    "    r2 = (\"2017-12-25 12:01:00\", \"note\", 1000)\n",
    "    r3 = (\"2017-12-25 12:01:10\", \"pencil\", 3500)\n",
    "    r4 = (\"2017-12-25 12:03:20\", \"pencil\", 23000)\n",
    "    r5 = (\"2017-12-25 12:05:00\", \"note\", 1500)\n",
    "    r6 = (\"2017-12-25 12:05:07\", \"note\", 2000)\n",
    "    r7 = (\"2017-12-25 12:06:25\", \"note\", 1000)\n",
    "    r8 = (\"2017-12-25 12:08:00\", \"pencil\", 500)\n",
    "    r9 = (\"2017-12-25 12:09:45\", \"note\", 30000)\n",
    "\n",
    "    dd = spark.createDataFrame([r2, r3, r4, r5, r6, r7, r8, r9], schema2);\n",
    "\n",
    "    timeCol = unix_timestamp(dd[\"date\"]).cast(\"timestamp\");\n",
    "    windowCol = window(timeCol, \"5 minutes\");\n",
    "    dd.groupBy(windowCol, dd[\"product\"]).agg(sum(dd[\"amount\"])).show(truncate=False);\n",
    "\n",
    "\n",
    "# 5.5.2.4.15절\n",
    "def runDateFunctions(spark):\n",
    "    # 파이썬의 경우 아래와 같이 튜플을 이용하여 데이터프레임을 생성하는 것도 가능함\n",
    "    df1 = spark.createDataFrame([(1.512,), (2.234,), (3.42,)], ['value'])\n",
    "    df2 = spark.createDataFrame([(25.0,), (9.0,), (10.0,)], ['value'])\n",
    "\n",
    "    df1.select(round(df1[\"value\"], 1)).show()\n",
    "    df2.select(functions.sqrt('value')).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.16 ~ 5.5.2.4.20절\n",
    "def runOtherFunctions(spark, personDf):\n",
    "    df = spark.createDataFrame([(\"v1\", \"v2\", \"v3\")], [\"c1\", \"c2\", \"c3\"]);\n",
    "\n",
    "    # array\n",
    "    df.select(df.c1, df.c2, df.c3, array(\"c1\", \"c2\", \"c3\").alias(\"newCol\")).show(truncate=False)\n",
    "\n",
    "    # desc, asc\n",
    "    personDf.show()\n",
    "    personDf.sort(functions.desc(\"age\"), functions.asc(\"name\")).show()\n",
    "\n",
    "    # pyspark 2.1.0 버전은 desc_nulls_first, desc_nulls_last, asc_nulls_first, asc_nulls_last 지원하지 않음\n",
    "\n",
    "    # split, length (pyspark에서 컬럼은 df[\"col\"] 또는 df.col 형태로 사용 가능)\n",
    "    df2 = spark.createDataFrame([(\"Splits str around pattern\",)], ['value'])\n",
    "    df2.select(df2.value, split(df2.value, \" \"), length(df2.value)).show(truncate=False)\n",
    "\n",
    "    # rownum, rank\n",
    "    f1 = StructField(\"date\", StringType(), True)\n",
    "    f2 = StructField(\"product\", StringType(), True)\n",
    "    f3 = StructField(\"amount\", IntegerType(), True)\n",
    "    schema = StructType([f1, f2, f3])\n",
    "\n",
    "    p1 = (\"2017-12-25 12:01:00\", \"note\", 1000)\n",
    "    p2 = (\"2017-12-25 12:01:10\", \"pencil\", 3500)\n",
    "    p3 = (\"2017-12-25 12:03:20\", \"pencil\", 23000)\n",
    "    p4 = (\"2017-12-25 12:05:00\", \"note\", 1500)\n",
    "    p5 = (\"2017-12-25 12:05:07\", \"note\", 2000)\n",
    "    p6 = (\"2017-12-25 12:06:25\", \"note\", 1000)\n",
    "    p7 = (\"2017-12-25 12:08:00\", \"pencil\", 500)\n",
    "    p8 = (\"2017-12-25 12:09:45\", \"note\", 30000)\n",
    "\n",
    "    dd = spark.createDataFrame([p1, p2, p3, p4, p5, p6, p7, p8], schema)\n",
    "    w1 = Window.partitionBy(\"product\").orderBy(\"amount\")\n",
    "    w2 = Window.orderBy(\"amount\")\n",
    "    dd.select(dd.product, dd.amount, functions.row_number().over(w1).alias(\"rownum\"),\n",
    "              functions.rank().over(w2).alias(\"rank\")).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.21절\n",
    "def runUDF(spark, df):\n",
    "    # functions를 이용한 등록\n",
    "    fn1 = functions.udf(lambda job: job == \"student\")\n",
    "    df.select(df[\"name\"], df[\"age\"], df[\"job\"], fn1(df[\"job\"])).show()\n",
    "    # SparkSession을 이용한 등록\n",
    "    spark.udf.register(\"fn2\", lambda job: job == \"student\")\n",
    "    df.createOrReplaceTempView(\"persons\")\n",
    "    spark.sql(\"select name, age, job, fn2(job) from persons\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.24절\n",
    "def runAgg(spark, df):\n",
    "    df.agg(max(\"amount\"), min(\"price\")).show()\n",
    "    df.agg({\"amount\": \"max\", \"price\": \"min\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.26절\n",
    "def runDfAlias(spark, df):\n",
    "    df.select(df[\"product\"]).show()\n",
    "    df.alias(\"aa\").select(\"aa.product\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.27절\n",
    "def runGroupBy(spark, df):\n",
    "    df.groupBy(\"store\", \"product\").agg({\"price\": \"sum\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.3.4.28절\n",
    "def runCube(spark, df):\n",
    "    df.cube(\"store\", \"product\").agg({\"price\": \"sum\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.29절\n",
    "def runDistinct(spark):\n",
    "    d1 = (\"store1\", \"note\", 20, 2000)\n",
    "    d2 = (\"store1\", \"bag\", 10, 5000)\n",
    "    d3 = (\"store1\", \"note\", 20, 2000)\n",
    "    rows = [d1, d2, d3]\n",
    "    cols = [\"store\", \"product\", \"amount\", \"price\"]\n",
    "    df = spark.createDataFrame(rows, cols)\n",
    "    df.distinct().show()\n",
    "    df.dropDuplicates([\"store\"]).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.30절\n",
    "def runDrop(spark, df):\n",
    "    df.drop(df[\"store\"]).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.31절\n",
    "def runIntersect(spark):\n",
    "    a = spark.range(1, 5)\n",
    "    b = spark.range(2, 6)\n",
    "    c = a.intersect(b)\n",
    "    c.show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.32절\n",
    "def runExcept(spark):\n",
    "    df1 = spark.range(1, 6)\n",
    "    df2 = spark.createDataFrame([(2,), (4,)], ['value'])\n",
    "    # 파이썬의 경우 except 대신 subtract 메서드 사용\n",
    "    # subtract의 동작은 except와 같음\n",
    "    df1.subtract(df2).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.33절\n",
    "def runJoin(spark, ldf, rdf):\n",
    "    joinTypes = \"inner,outer,leftouter,rightouter,leftsemi\".split(\",\")\n",
    "    for joinType in joinTypes:\n",
    "        print(\"============= %s ===============\" % joinType)\n",
    "        ldf.join(rdf, [\"word\"], joinType).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.35절\n",
    "def runNa(spark, ldf, rdf):\n",
    "    result = ldf.join(rdf, [\"word\"], \"outer\").toDF(\"word\", \"c1\", \"c2\")\n",
    "    result.show()\n",
    "    # 파이썬의 경우 na.drop또는 dropna 사용 가능\n",
    "    # c1과 c2 칼럼의 null이 아닌 값의 개수가 thresh 이하일 경우 drop\n",
    "    # thresh=1로 설정할 경우 c1 또는 c2 둘 중의 하나만 null 아닌 값을 가질 경우\n",
    "    # 결과에 포함시킨다는 의미가 됨\n",
    "    result.na.drop(thresh=2, subset=[\"c1\", \"c2\"]).show()\n",
    "    result.dropna(thresh=2, subset=[\"c1\", \"c2\"]).show()\n",
    "    # fill\n",
    "    result.na.fill({\"c1\": 0}).show()\n",
    "    # 파이썬의 경우 to_replace에 딕셔너리를 지정하여 replace를 수행(이 경우 value에 선언한 값은 무시됨\n",
    "    # 딕셔너리를 사용하지 않을 경우 키 목록(첫번째 인자)과 값 목록(두번째 인자)을 지정하여 replace 수행\n",
    "    result.na.replace(to_replace={\"w1\": \"word1\", \"w2\": \"word2\"}, value=\"\", subset=\"word\").show()\n",
    "    result.na.replace([\"w1\", \"w2\"], [\"word1\", \"word2\"], \"word\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.36절\n",
    "def runOrderBy(spark):\n",
    "    df = spark.createDataFrame([(3, \"z\"), (10, \"a\"), (5, \"c\")], [\"idx\", \"name\"])\n",
    "    df.orderBy(\"name\", \"idx\").show()\n",
    "    df.orderBy(\"idx\", \"name\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.37절\n",
    "def runRollup(spark, df):\n",
    "    df.rollup(\"store\", \"product\").agg({\"price\": \"sum\"}).show();\n",
    "\n",
    "\n",
    "# 5.5.2.4.38절\n",
    "def runStat(spark):\n",
    "    df = spark.createDataFrame([(\"a\", 6), (\"b\", 4), (\"c\", 12), (\"d\", 6)], [\"word\", \"count\"])\n",
    "    df.show()\n",
    "    df.stat.crosstab(\"word\", \"count\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.39절\n",
    "def runWithColumn(spark):\n",
    "    df1 = spark.createDataFrame([(\"prod1\", \"100\"), (\"prod2\", \"200\")], [\"pname\", \"price\"])\n",
    "    df2 = df1.withColumn(\"dcprice\", df1[\"price\"] * 0.9)\n",
    "    df3 = df2.withColumnRenamed(\"dcprice\", \"newprice\")\n",
    "    df1.show()\n",
    "    df2.show()\n",
    "    df3.show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.40절\n",
    "def runSave(spark):\n",
    "    sparkHomeDir = \"file:///Users/beginspark/Apps/spark\"\n",
    "    df = spark.read.json(sparkHomeDir + \"/examples/src/main/resources/people.json\")\n",
    "    df.write.save(\"/Users/beginspark/Temp/default/%d\" % time.time())\n",
    "    df.write.format(\"json\").save(\"/Users/beginspark/Temp/json/%d\" % time.time())\n",
    "    df.write.format(\"json\").partitionBy(\"age\").save(\"/Users/beginspark/Temp/parti/%d\" % time.time())\n",
    "    # saveMode: append, overwrite, error, ignore\n",
    "    df.write.mode(\"overwrite\").saveAsTable(\"ohMyTable\")\n",
    "    spark.sql(\"select * from ohMyTable\").show()\n",
    "    # 파이썬의 경우 bucketBy 지원하지 않음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [예제 실행 방법] 아래에서 원하는 예제의 주석을 제거하고 실행!!\n",
    "\n",
    "# runBasicOpsEx(spark, sc, sample_df)\n",
    "# createDataFrame(spark, sc)\n",
    "#runColumnEx(spark, sc, sample_df)\n",
    "# runAlias(spark, sc, sample_df)\n",
    "# runIsinEx(spark, sc)\n",
    "# runWhenEx(spark, sc)\n",
    "# runMaxMin(spark, sample_df)\n",
    "# runAggregateFunctions(spark, sample_df, sample_df2)\n",
    "# runCollectionFunctions(spark)\n",
    "# runDateFunctions(spark)\n",
    "# runDateFunctions(spark)\n",
    "# runOtherFunctions(spark, sample_df)\n",
    "# runUDF(spark, sample_df)\n",
    "# runAgg(spark, sample_df2)\n",
    "# runDfAlias(spark, sample_df2)\n",
    "# runGroupBy(spark, sample_df2)\n",
    "# runCube(spark, sample_df2)\n",
    "# runDistinct(spark)\n",
    "# runDrop(spark, sample_df2)\n",
    "# runIntersect(spark)\n",
    "# runExcept(spark)\n",
    "# runJoin(spark, ldf, rdf)\n",
    "# runNa(spark, ldf, rdf)\n",
    "# runOrderBy(spark)\n",
    "# runRollup(spark, sample_df2)\n",
    "# runWithColumn(spark)\n",
    "# runSave(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
