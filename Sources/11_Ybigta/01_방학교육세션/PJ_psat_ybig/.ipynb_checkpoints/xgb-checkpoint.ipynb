{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") #Needed to save figures\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-1) ** 2))\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)\n",
    "\n",
    "# Gather some features\n",
    "def build_features(features, data):\n",
    "    # remove NaNs\n",
    "    #data.fillna(0, inplace=True)\n",
    "    data.loc[data.Open.isnull(), 'Open'] = 1\n",
    "    # Use some properties directly\n",
    "    features.extend(['Store', 'CompetitionDistance', 'Promo', 'Promo2', 'SchoolHoliday'])\n",
    "\n",
    "    # Label encode some features\n",
    "    features.extend(['StoreType', 'Assortment', 'StateHoliday'])\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "\n",
    "    features.extend(['DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear'])\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['DayOfWeek'] = data.Date.dt.dayofweek\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "\n",
    "    # CompetionOpen en PromoOpen from https://www.kaggle.com/ananya77041/rossmann-store-sales/randomforestpython/code\n",
    "    # Calculate time competition open time in months\n",
    "    features.append('CompetitionOpen')\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n",
    "        (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    # Promo open time in months\n",
    "    features.append('PromoOpen')\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n",
    "        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "    data.loc[data.Promo2SinceYear == 0, 'PromoOpen'] = 0\n",
    "\n",
    "    # Indicate that sales on that day are in promo interval\n",
    "    features.append('IsPromoMonth')\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    data['monthStr'] = data.Month.map(month2str)\n",
    "    data.loc[data.PromoInterval == 0, 'PromoInterval'] = ''\n",
    "    data['IsPromoMonth'] = 0\n",
    "    for interval in data.PromoInterval.unique():\n",
    "        if interval != '':\n",
    "            for month in interval.split(','):\n",
    "                data.loc[(data.monthStr == month) & (data.PromoInterval == interval), 'IsPromoMonth'] = 1\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training, test and store data using pandas\n",
      "Assume store open, if not provided\n",
      "Consider only open stores for training. Closed stores wont count into the score.\n",
      "Use only Sales bigger then zero. Simplifies calculation of rmspe\n",
      "Join with store\n",
      "augment features\n",
      "['Store', 'CompetitionDistance', 'Promo', 'Promo2', 'SchoolHoliday', 'StoreType', 'Assortment', 'StateHoliday', 'DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n",
      "training data processed\n"
     ]
    }
   ],
   "source": [
    "## Start of main script\n",
    "\n",
    "print(\"Load the training, test and store data using pandas\")\n",
    "types = {'CompetitionOpenSinceYear': np.dtype(int),\n",
    "         'CompetitionOpenSinceMonth': np.dtype(int),\n",
    "         'StateHoliday': np.dtype(str),\n",
    "         'Promo2SinceWeek': np.dtype(int),\n",
    "         'SchoolHoliday': np.dtype(float),\n",
    "         'PromoInterval': np.dtype(str)}\n",
    "train = pd.read_csv(\"Drugstore_data/train.csv\", parse_dates=[2], dtype=types)\n",
    "test = pd.read_csv(\"Drugstore_data/test.csv\", parse_dates=[3], dtype=types)\n",
    "store = pd.read_csv(\"Drugstore_data/store.csv\")\n",
    "\n",
    "print(\"Assume store open, if not provided\")\n",
    "train.fillna(1, inplace=True)\n",
    "test.fillna(1, inplace=True)\n",
    "\n",
    "print(\"Consider only open stores for training. Closed stores wont count into the score.\")\n",
    "train = train[train[\"Open\"] != 0]\n",
    "print(\"Use only Sales bigger then zero. Simplifies calculation of rmspe\")\n",
    "train = train[train[\"Sales\"] > 0]\n",
    "\n",
    "print(\"Join with store\")\n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')\n",
    "\n",
    "features = []\n",
    "\n",
    "print(\"augment features\")\n",
    "build_features(features, train)\n",
    "build_features([], test)\n",
    "print(features)\n",
    "\n",
    "print('training data processed')\n",
    "\n",
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"booster\" : \"gbtree\",\n",
    "          \"eta\": 0.2,\n",
    "          \"max_depth\": 30,\n",
    "          \"subsample\": 0.9,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"silent\": 1,\n",
    "          \"seed\": 1301\n",
    "          }\n",
    "num_boost_round = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a XGBoost model\n"
     ]
    }
   ],
   "source": [
    "print(\"Train a XGBoost model\")\n",
    "\n",
    "X_train = train[(train['Date'] <= '2014-06-01') | (train['Date'] >= '2014-08-01')]\n",
    "X_valid = train[(train['Date'] > '2014-06-01') & (train['Date']< '2014-08-01')]\n",
    "y_train = np.log1p(X_train.Sales)\n",
    "y_valid = np.log1p(X_valid.Sales)\n",
    "dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "dvalid = xgb.DMatrix(X_valid[features], y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?xgb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:6.61558\teval-rmse:6.64013\ttrain-rmspe:0.998701\teval-rmspe:0.998738\n",
      "Multiple eval metrics have been passed: 'eval-rmspe' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmspe hasn't improved in 100 rounds.\n",
      "[1]\ttrain-rmse:5.29647\teval-rmse:5.32193\ttrain-rmspe:0.994711\teval-rmspe:0.994871\n",
      "[2]\ttrain-rmse:4.2414\teval-rmse:4.26692\ttrain-rmspe:0.984664\teval-rmspe:0.985118\n",
      "[3]\ttrain-rmse:3.3982\teval-rmse:3.42414\ttrain-rmspe:0.964315\teval-rmspe:0.965424\n",
      "[4]\ttrain-rmse:2.72292\teval-rmse:2.74769\ttrain-rmspe:0.93062\teval-rmspe:0.932645\n",
      "[5]\ttrain-rmse:2.183\teval-rmse:2.2063\ttrain-rmspe:0.881946\teval-rmspe:0.885035\n",
      "[6]\ttrain-rmse:1.75262\teval-rmse:1.77819\ttrain-rmspe:0.81893\teval-rmspe:0.823995\n",
      "[7]\ttrain-rmse:1.40703\teval-rmse:1.43068\ttrain-rmspe:0.746477\teval-rmspe:0.752739\n",
      "[8]\ttrain-rmse:1.13138\teval-rmse:1.15458\ttrain-rmspe:0.668214\teval-rmspe:0.675879\n",
      "[9]\ttrain-rmse:0.912375\teval-rmse:0.935292\ttrain-rmspe:0.588342\teval-rmspe:0.597154\n",
      "[10]\ttrain-rmse:0.73558\teval-rmse:0.757597\ttrain-rmspe:0.512085\teval-rmspe:0.521012\n",
      "[11]\ttrain-rmse:0.596363\teval-rmse:0.619316\ttrain-rmspe:0.441673\teval-rmspe:0.450879\n",
      "[12]\ttrain-rmse:0.485914\teval-rmse:0.510296\ttrain-rmspe:0.378848\teval-rmspe:0.388532\n",
      "[13]\ttrain-rmse:0.399293\teval-rmse:0.426024\ttrain-rmspe:0.321925\teval-rmspe:0.335423\n",
      "[14]\ttrain-rmse:0.326652\teval-rmse:0.355701\ttrain-rmspe:0.273722\teval-rmspe:0.288873\n",
      "[15]\ttrain-rmse:0.272359\teval-rmse:0.302706\ttrain-rmspe:0.232926\teval-rmspe:0.251387\n",
      "[16]\ttrain-rmse:0.229131\teval-rmse:0.26413\ttrain-rmspe:0.200082\teval-rmspe:0.223254\n",
      "[17]\ttrain-rmse:0.192992\teval-rmse:0.232003\ttrain-rmspe:0.17137\teval-rmspe:0.199594\n",
      "[18]\ttrain-rmse:0.163283\teval-rmse:0.204352\ttrain-rmspe:0.148033\teval-rmspe:0.178931\n",
      "[19]\ttrain-rmse:0.1394\teval-rmse:0.186817\ttrain-rmspe:0.127945\teval-rmspe:0.165853\n",
      "[20]\ttrain-rmse:0.121683\teval-rmse:0.172535\ttrain-rmspe:0.113157\teval-rmspe:0.155253\n",
      "[21]\ttrain-rmse:0.106201\teval-rmse:0.161137\ttrain-rmspe:0.099989\teval-rmspe:0.146835\n",
      "[22]\ttrain-rmse:0.093071\teval-rmse:0.154857\ttrain-rmspe:0.088205\teval-rmspe:0.142536\n",
      "[23]\ttrain-rmse:0.081924\teval-rmse:0.149718\ttrain-rmspe:0.078077\teval-rmspe:0.139074\n",
      "[24]\ttrain-rmse:0.072259\teval-rmse:0.145737\ttrain-rmspe:0.069258\teval-rmspe:0.136623\n",
      "[25]\ttrain-rmse:0.063987\teval-rmse:0.142324\ttrain-rmspe:0.061638\teval-rmspe:0.134385\n",
      "[26]\ttrain-rmse:0.058249\teval-rmse:0.139483\ttrain-rmspe:0.056448\teval-rmspe:0.132576\n",
      "[27]\ttrain-rmse:0.054002\teval-rmse:0.138065\ttrain-rmspe:0.05256\teval-rmspe:0.132004\n",
      "[28]\ttrain-rmse:0.048838\teval-rmse:0.136936\ttrain-rmspe:0.047674\teval-rmspe:0.131524\n",
      "[29]\ttrain-rmse:0.044435\teval-rmse:0.135852\ttrain-rmspe:0.043511\teval-rmspe:0.131032\n",
      "[30]\ttrain-rmse:0.040537\teval-rmse:0.134842\ttrain-rmspe:0.039817\teval-rmspe:0.130455\n",
      "[31]\ttrain-rmse:0.037366\teval-rmse:0.133964\ttrain-rmspe:0.036813\teval-rmspe:0.129951\n",
      "[32]\ttrain-rmse:0.035297\teval-rmse:0.133211\ttrain-rmspe:0.034865\teval-rmspe:0.129533\n",
      "[33]\ttrain-rmse:0.032403\teval-rmse:0.132925\ttrain-rmspe:0.03206\teval-rmspe:0.129516\n",
      "[34]\ttrain-rmse:0.030801\teval-rmse:0.132831\ttrain-rmspe:0.030531\teval-rmspe:0.129617\n",
      "[35]\ttrain-rmse:0.028703\teval-rmse:0.132462\ttrain-rmspe:0.028495\teval-rmspe:0.129425\n",
      "[36]\ttrain-rmse:0.02679\teval-rmse:0.132248\ttrain-rmspe:0.026627\teval-rmspe:0.129364\n",
      "[37]\ttrain-rmse:0.024808\teval-rmse:0.132091\ttrain-rmspe:0.024676\teval-rmspe:0.129337\n",
      "[38]\ttrain-rmse:0.022874\teval-rmse:0.131951\ttrain-rmspe:0.022766\teval-rmspe:0.129294\n",
      "[39]\ttrain-rmse:0.022475\teval-rmse:0.131726\ttrain-rmspe:0.022392\teval-rmspe:0.129174\n",
      "[40]\ttrain-rmse:0.02203\teval-rmse:0.131574\ttrain-rmspe:0.021972\teval-rmspe:0.129122\n",
      "[41]\ttrain-rmse:0.020432\teval-rmse:0.131496\ttrain-rmspe:0.020384\teval-rmspe:0.129106\n",
      "[42]\ttrain-rmse:0.019433\teval-rmse:0.131422\ttrain-rmspe:0.019394\teval-rmspe:0.129083\n",
      "[43]\ttrain-rmse:0.018439\teval-rmse:0.131343\ttrain-rmspe:0.01841\teval-rmspe:0.129046\n",
      "[44]\ttrain-rmse:0.017347\teval-rmse:0.131321\ttrain-rmspe:0.017323\teval-rmspe:0.129057\n",
      "[45]\ttrain-rmse:0.016192\teval-rmse:0.13133\ttrain-rmspe:0.016171\teval-rmspe:0.129106\n",
      "[46]\ttrain-rmse:0.0153\teval-rmse:0.131318\ttrain-rmspe:0.015284\teval-rmspe:0.129106\n",
      "[47]\ttrain-rmse:0.014636\teval-rmse:0.131308\ttrain-rmspe:0.014624\teval-rmspe:0.129126\n",
      "[48]\ttrain-rmse:0.013973\teval-rmse:0.131295\ttrain-rmspe:0.013963\teval-rmspe:0.129135\n",
      "[49]\ttrain-rmse:0.013279\teval-rmse:0.131276\ttrain-rmspe:0.013271\teval-rmspe:0.129137\n",
      "[50]\ttrain-rmse:0.012546\teval-rmse:0.131288\ttrain-rmspe:0.012541\teval-rmspe:0.129165\n",
      "[51]\ttrain-rmse:0.01188\teval-rmse:0.131272\ttrain-rmspe:0.011877\teval-rmspe:0.129163\n",
      "[52]\ttrain-rmse:0.011127\teval-rmse:0.13127\ttrain-rmspe:0.011124\teval-rmspe:0.129176\n",
      "[53]\ttrain-rmse:0.010538\teval-rmse:0.131266\ttrain-rmspe:0.010536\teval-rmspe:0.129174\n",
      "[54]\ttrain-rmse:0.009976\teval-rmse:0.131252\ttrain-rmspe:0.009975\teval-rmspe:0.129169\n",
      "[55]\ttrain-rmse:0.009485\teval-rmse:0.131263\ttrain-rmspe:0.009483\teval-rmspe:0.129183\n",
      "[56]\ttrain-rmse:0.008771\teval-rmse:0.131261\ttrain-rmspe:0.008769\teval-rmspe:0.129187\n",
      "[57]\ttrain-rmse:0.00817\teval-rmse:0.131277\ttrain-rmspe:0.008168\teval-rmspe:0.129212\n",
      "[58]\ttrain-rmse:0.00778\teval-rmse:0.131294\ttrain-rmspe:0.007779\teval-rmspe:0.12923\n",
      "[59]\ttrain-rmse:0.007382\teval-rmse:0.131283\ttrain-rmspe:0.007382\teval-rmspe:0.129219\n",
      "[60]\ttrain-rmse:0.007172\teval-rmse:0.131289\ttrain-rmspe:0.007172\teval-rmspe:0.129234\n",
      "[61]\ttrain-rmse:0.006581\teval-rmse:0.131286\ttrain-rmspe:0.006581\teval-rmspe:0.12923\n",
      "[62]\ttrain-rmse:0.006143\teval-rmse:0.131281\ttrain-rmspe:0.006143\teval-rmspe:0.12923\n",
      "[63]\ttrain-rmse:0.005726\teval-rmse:0.131277\ttrain-rmspe:0.005726\teval-rmspe:0.129228\n",
      "[64]\ttrain-rmse:0.005398\teval-rmse:0.131273\ttrain-rmspe:0.005398\teval-rmspe:0.129224\n",
      "[65]\ttrain-rmse:0.005287\teval-rmse:0.131263\ttrain-rmspe:0.005287\teval-rmspe:0.129221\n",
      "[66]\ttrain-rmse:0.004981\teval-rmse:0.131261\ttrain-rmspe:0.004981\teval-rmspe:0.12922\n",
      "[67]\ttrain-rmse:0.004712\teval-rmse:0.131272\ttrain-rmspe:0.004712\teval-rmspe:0.129231\n",
      "[68]\ttrain-rmse:0.004372\teval-rmse:0.131275\ttrain-rmspe:0.004373\teval-rmspe:0.129235\n",
      "[69]\ttrain-rmse:0.00408\teval-rmse:0.131282\ttrain-rmspe:0.00408\teval-rmspe:0.129243\n",
      "[70]\ttrain-rmse:0.004036\teval-rmse:0.131282\ttrain-rmspe:0.004037\teval-rmspe:0.129243\n",
      "[71]\ttrain-rmse:0.003778\teval-rmse:0.131294\ttrain-rmspe:0.003779\teval-rmspe:0.129251\n",
      "[72]\ttrain-rmse:0.00356\teval-rmse:0.131297\ttrain-rmspe:0.003561\teval-rmspe:0.129255\n",
      "[73]\ttrain-rmse:0.003342\teval-rmse:0.131302\ttrain-rmspe:0.003342\teval-rmspe:0.129259\n",
      "[74]\ttrain-rmse:0.00321\teval-rmse:0.131299\ttrain-rmspe:0.003211\teval-rmspe:0.129256\n",
      "[75]\ttrain-rmse:0.003019\teval-rmse:0.131305\ttrain-rmspe:0.003019\teval-rmspe:0.129265\n",
      "[76]\ttrain-rmse:0.002849\teval-rmse:0.131311\ttrain-rmspe:0.002849\teval-rmspe:0.12927\n",
      "[77]\ttrain-rmse:0.002677\teval-rmse:0.131316\ttrain-rmspe:0.002678\teval-rmspe:0.129275\n",
      "[78]\ttrain-rmse:0.002559\teval-rmse:0.131317\ttrain-rmspe:0.002559\teval-rmspe:0.129276\n",
      "[79]\ttrain-rmse:0.002406\teval-rmse:0.131321\ttrain-rmspe:0.002407\teval-rmspe:0.129281\n",
      "[80]\ttrain-rmse:0.002211\teval-rmse:0.13132\ttrain-rmspe:0.002211\teval-rmspe:0.129281\n",
      "[81]\ttrain-rmse:0.002094\teval-rmse:0.131319\ttrain-rmspe:0.002094\teval-rmspe:0.12928\n",
      "[82]\ttrain-rmse:0.001957\teval-rmse:0.131318\ttrain-rmspe:0.001957\teval-rmspe:0.129281\n",
      "[83]\ttrain-rmse:0.001831\teval-rmse:0.131318\ttrain-rmspe:0.001832\teval-rmspe:0.129282\n",
      "[84]\ttrain-rmse:0.001721\teval-rmse:0.13132\ttrain-rmspe:0.001722\teval-rmspe:0.129284\n",
      "[85]\ttrain-rmse:0.001706\teval-rmse:0.131318\ttrain-rmspe:0.001706\teval-rmspe:0.129282\n",
      "[86]\ttrain-rmse:0.001596\teval-rmse:0.131319\ttrain-rmspe:0.001596\teval-rmspe:0.129284\n",
      "[87]\ttrain-rmse:0.001553\teval-rmse:0.131319\ttrain-rmspe:0.001553\teval-rmspe:0.129284\n",
      "[88]\ttrain-rmse:0.00145\teval-rmse:0.13132\ttrain-rmspe:0.00145\teval-rmspe:0.129285\n",
      "[89]\ttrain-rmse:0.001392\teval-rmse:0.13132\ttrain-rmspe:0.001393\teval-rmspe:0.129286\n",
      "[90]\ttrain-rmse:0.001336\teval-rmse:0.131321\ttrain-rmspe:0.001336\teval-rmspe:0.129287\n",
      "[91]\ttrain-rmse:0.001279\teval-rmse:0.131321\ttrain-rmspe:0.001279\teval-rmspe:0.129287\n",
      "[92]\ttrain-rmse:0.001242\teval-rmse:0.131322\ttrain-rmspe:0.001242\teval-rmspe:0.129289\n",
      "[93]\ttrain-rmse:0.00123\teval-rmse:0.131323\ttrain-rmspe:0.00123\teval-rmspe:0.129289\n",
      "[94]\ttrain-rmse:0.001218\teval-rmse:0.131323\ttrain-rmspe:0.001219\teval-rmspe:0.129289\n",
      "[95]\ttrain-rmse:0.001217\teval-rmse:0.131322\ttrain-rmspe:0.001217\teval-rmspe:0.129289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96]\ttrain-rmse:0.001197\teval-rmse:0.131323\ttrain-rmspe:0.001197\teval-rmspe:0.129289\n",
      "[97]\ttrain-rmse:0.001197\teval-rmse:0.131323\ttrain-rmspe:0.001197\teval-rmspe:0.129289\n",
      "[98]\ttrain-rmse:0.001197\teval-rmse:0.131323\ttrain-rmspe:0.001197\teval-rmspe:0.129289\n",
      "[99]\ttrain-rmse:0.001191\teval-rmse:0.131323\ttrain-rmspe:0.001191\teval-rmspe:0.12929\n",
      "[100]\ttrain-rmse:0.001176\teval-rmse:0.131323\ttrain-rmspe:0.001177\teval-rmspe:0.129289\n",
      "[101]\ttrain-rmse:0.001176\teval-rmse:0.131323\ttrain-rmspe:0.001177\teval-rmspe:0.129289\n",
      "[102]\ttrain-rmse:0.001176\teval-rmse:0.131323\ttrain-rmspe:0.001177\teval-rmspe:0.129289\n",
      "[103]\ttrain-rmse:0.001176\teval-rmse:0.131323\ttrain-rmspe:0.001177\teval-rmspe:0.129289\n",
      "[104]\ttrain-rmse:0.00116\teval-rmse:0.131323\ttrain-rmspe:0.00116\teval-rmspe:0.129289\n",
      "[105]\ttrain-rmse:0.001151\teval-rmse:0.131323\ttrain-rmspe:0.001151\teval-rmspe:0.12929\n",
      "[106]\ttrain-rmse:0.001151\teval-rmse:0.131323\ttrain-rmspe:0.001151\teval-rmspe:0.12929\n",
      "[107]\ttrain-rmse:0.001151\teval-rmse:0.131323\ttrain-rmspe:0.001151\teval-rmspe:0.12929\n",
      "[108]\ttrain-rmse:0.001151\teval-rmse:0.131323\ttrain-rmspe:0.001151\teval-rmspe:0.12929\n",
      "[109]\ttrain-rmse:0.001151\teval-rmse:0.131323\ttrain-rmspe:0.001151\teval-rmspe:0.12929\n",
      "[110]\ttrain-rmse:0.00114\teval-rmse:0.131323\ttrain-rmspe:0.00114\teval-rmspe:0.12929\n",
      "[111]\ttrain-rmse:0.001136\teval-rmse:0.131323\ttrain-rmspe:0.001136\teval-rmspe:0.12929\n",
      "[112]\ttrain-rmse:0.001135\teval-rmse:0.131323\ttrain-rmspe:0.001135\teval-rmspe:0.12929\n",
      "[113]\ttrain-rmse:0.001134\teval-rmse:0.131323\ttrain-rmspe:0.001135\teval-rmspe:0.12929\n",
      "[114]\ttrain-rmse:0.001134\teval-rmse:0.131323\ttrain-rmspe:0.001135\teval-rmspe:0.12929\n",
      "[115]\ttrain-rmse:0.001112\teval-rmse:0.131323\ttrain-rmspe:0.001112\teval-rmspe:0.12929\n",
      "[116]\ttrain-rmse:0.001112\teval-rmse:0.131323\ttrain-rmspe:0.001112\teval-rmspe:0.12929\n",
      "[117]\ttrain-rmse:0.001112\teval-rmse:0.131323\ttrain-rmspe:0.001112\teval-rmspe:0.12929\n",
      "[118]\ttrain-rmse:0.001112\teval-rmse:0.131323\ttrain-rmspe:0.001112\teval-rmspe:0.12929\n",
      "[119]\ttrain-rmse:0.001112\teval-rmse:0.131323\ttrain-rmspe:0.001112\teval-rmspe:0.12929\n",
      "[120]\ttrain-rmse:0.001112\teval-rmse:0.131323\ttrain-rmspe:0.001112\teval-rmspe:0.12929\n",
      "[121]\ttrain-rmse:0.001102\teval-rmse:0.131323\ttrain-rmspe:0.001102\teval-rmspe:0.12929\n",
      "[122]\ttrain-rmse:0.001102\teval-rmse:0.131323\ttrain-rmspe:0.001102\teval-rmspe:0.12929\n",
      "[123]\ttrain-rmse:0.001102\teval-rmse:0.131323\ttrain-rmspe:0.001102\teval-rmspe:0.12929\n",
      "[124]\ttrain-rmse:0.001102\teval-rmse:0.131323\ttrain-rmspe:0.001102\teval-rmspe:0.12929\n",
      "[125]\ttrain-rmse:0.001102\teval-rmse:0.131323\ttrain-rmspe:0.001102\teval-rmspe:0.12929\n",
      "[126]\ttrain-rmse:0.001102\teval-rmse:0.131323\ttrain-rmspe:0.001102\teval-rmspe:0.12929\n",
      "[127]\ttrain-rmse:0.001102\teval-rmse:0.131323\ttrain-rmspe:0.001102\teval-rmspe:0.12929\n",
      "[128]\ttrain-rmse:0.001102\teval-rmse:0.131323\ttrain-rmspe:0.001102\teval-rmspe:0.12929\n",
      "[129]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[130]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[131]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[132]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[133]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[134]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[135]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[136]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[137]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[138]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[139]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[140]\ttrain-rmse:0.001091\teval-rmse:0.131323\ttrain-rmspe:0.001092\teval-rmspe:0.129289\n",
      "[141]\ttrain-rmse:0.001087\teval-rmse:0.131323\ttrain-rmspe:0.001087\teval-rmspe:0.129289\n",
      "[142]\ttrain-rmse:0.001086\teval-rmse:0.131323\ttrain-rmspe:0.001086\teval-rmspe:0.129289\n",
      "[143]\ttrain-rmse:0.001086\teval-rmse:0.131323\ttrain-rmspe:0.001086\teval-rmspe:0.129289\n",
      "Stopping. Best iteration:\n",
      "[43]\ttrain-rmse:0.018439\teval-rmse:0.131343\ttrain-rmspe:0.01841\teval-rmspe:0.129046\n",
      "\n",
      "Validating\n",
      "RMSPE: 0.129289\n",
      "Make predictions on the test set\n"
     ]
    }
   ],
   "source": [
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\n",
    "\n",
    "print(\"Validating\")\n",
    "yhat = gbm.predict(xgb.DMatrix(X_valid[features]))\n",
    "error = rmspe(X_valid.Sales.values, np.expm1(yhat))\n",
    "print('RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "print(\"Make predictions on the test set\")\n",
    "dtest = xgb.DMatrix(test[features])\n",
    "test_probs = gbm.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\"Id\": test[\"ID\"], 'Sales': np.expm1(test_probs)})\n",
    "result.to_csv(\"xgboost_10_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.loc[test['Open']==0,'Sales'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_result = result.sort_values('Id').reset_index().drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16279381155105946"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(test_ans['Sales'],sorted_result['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
