}
else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}
if (data$total[i]<=8591){
data$total[i]='1st'
}
else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}
if (data$total[i]<=8591){
data$total[i]='1st'
}
else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}
if (data$total[i]<=8591){
data$total[i]='1st'
}
else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}
if (data$total[i]<=8591){
data$total[i]='1st'
}
if (data$total[i]<=8591){
data$total[i]='1st'
}
elif(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}
data = data.frame(total)
for (i in 1:nrow(data)){
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
} else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
} else {
data$total[i]='4th'
}
}
data
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
data
total = c(100,20000,10000,50000)
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
data
data = data.frame(total)
data
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
data
data = data.frame(total)
data
for (i in 1:nrow(data)){
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
}
data
i=2
data = data.frame(total)
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}
data
nrow(data)
1:nrow(data)
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
data
for i in nrow(data){print(i)}
for i in nrow(data){i}
for (i in nrow(data)){i}
for (i in nrow(data)){cat(i)}
for (i in 1:nrow(data)){cat(i)}
data = data.frame(total)
for (i in 1:nrow(data)){
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
}
data
'1st'<=8591
'4th'<=8591
'3rd'<=8591
total = c(100,20000,10000,50000)
data = data.frame(total)
for (i in 1:2)){
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
}
for (i in 1:2)){
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
}
for (i in 1:2))
for (i in (1:2))
{
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
}
data
data = data.frame(total)
for (i in 2)
{
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
}
data
data = data.frame(total)
for (i in 1:2){
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
}
data
if(a>1){print(11)} else if(a=2){print(22)}
if(a>1){print(11)} else if(a==2){print(22)}
a=2
if(a>1){print(11)} else if(a==2){print(22)}
for(i in 1:2){
if(a>1){print(11)} else if(a==2){print(22)}
}
for(a in 2:3){
if(a==2){print(11)} else if(a>=2){print(22)}
}
for(a in 1:3){
if(a==2){print(11)} else if(a>=2){print(22)} else{print(123)}
}
data = data.frame(total)
for (i in 1:nrow(data)){
if (data$total[i]<=8591){
data$total[i]='1st'
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
}else {
data$total[i]='4th'
}
print(i)
}
data = data.frame(total)
for (i in 1:nrow(data)){
if (data$total[i]<=8591){
data$total[i]='1st'
print('1st')
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
print('2st')
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
print('3st')
}else {
data$total[i]='4th'
print('rst')
}
print(i)
}
data = data.frame(total)
for (i in 1:nrow(data)){
if (data$total[i]<=8591){
data$total[i]='1st'
print(i)
print(data$total[i])
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
print('2st')
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
print('3st')
}else {
data$total[i]='4th'
print('rst')
}
print(i)
}
data = data.frame(total)
data
for (i in 1:nrow(data)){
if (data$total[i]<=8591){
print(data$total[i])
data$total[i]='1st'
print(i)
print(data$total[i])
}else if(8591<data$total[i]&data$total[i]<=19156){
data$total[i]='2nd'
print('2st')
}else if(19156<data$total[i]&data$total[i]<=46825){
data$total[i]='3rd'
print('3st')
}else {
data$total[i]='4th'
print('rst')
}
}
setwd('C:\Users\SUIC_STAR\Dropbox\Sources\Ybigta\PJ_psat_ybig\Origin\rossmann-store-sales')
setwd('C:\\Users\\SUIC_STAR\\Dropbox\\Sources\\Ybigta\\PJ_psat_ybig\\Origin\\rossmann-store-sales')
read.csv('train.csv')
train =read.csv('train.csv')
train$Date
max(train$Date)
install.packages('h2o')
as.Date(train$Date)
max(as.Date(train$Date))
library(data.table)
library(h2o)
cat("reading the train and test data (with data.table) \n")
train <- fread("train.csv",stringsAsFactors = T)
test  <- fread("test.csv",stringsAsFactors = T)
store <- fread("store.csv",stringsAsFactors = T)
train <- train[Sales > 0,]  ## We are not judged on 0 sales records in test set
## See Scripts discussion from 10/8 for more explanation.
train <- merge(train,store,by="Store")
test <- merge(test,store,by="Store")
summary('store')
summary(Store)
summary(store)
cat("train data column names and details\n")
summary(train)
cat("test data column names and details\n")
summary(test)
## more care should be taken to ensure the dates of test can be projected from train
## decision trees do not project well, so you will want to have some strategy here, if using the dates
train[,Date:=as.Date(Date)]
test[,Date:=as.Date(Date)]
# seperating out the elements of the date column for the train set
train[,month:=as.integer(format(Date, "%m"))]
train[,year:=as.integer(format(Date, "%y"))]
train[,Store:=as.factor(as.numeric(Store))]
test[,month:=as.integer(format(Date, "%m"))]
test[,year:=as.integer(format(Date, "%y"))]
test[,Store:=as.factor(as.numeric(Store))]
## log transformation to not be as sensitive to high sales
## decent rule of thumb:
##     if the data spans an order of magnitude, consider a log transform
train[,logSales:=log1p(Sales)]
## Use H2O's random forest
## Start cluster with all available threads
h2o.init(nthreads=-1,max_mem_size='6G')
## Load data into cluster from R
trainHex<-as.h2o(train)
## Set up variable to use all features other than those specified here
features<-colnames(train)[!(colnames(train) %in% c("Id","Date","Sales","logSales","Customers"))]
## Train a random forest using all default parameters
rfHex <- h2o.randomForest(x=features,
y="logSales",
ntrees = 100,
max_depth = 30,
nbins_cats = 1115, ## allow it to fit store ID
training_frame=trainHex)
summary(rfHex)
cat("Predicting Sales\n")
## Load test data into cluster from R
testHex<-as.h2o(test)
## Get predictions out; predicts in H2O, as.data.frame gets them into R
predictions<-as.data.frame(h2o.predict(rfHex,testHex))
## Return the predictions to the original scale of the Sales data
pred <- expm1(predictions[,1])
summary(pred)
submission <- data.frame(Id=test$Id, Sales=pred)
## Use H2O's random forest
## Start cluster with all available threads
h2o.init(nthreads=-1,max_mem_size='10G')
## Load data into cluster from R
trainHex<-as.h2o(train)
## Set up variable to use all features other than those specified here
features<-colnames(train)[!(colnames(train) %in% c("Id","Date","Sales","logSales","Customers"))]
## Train a random forest using all default parameters
rfHex <- h2o.randomForest(x=features,
y="logSales",
ntrees = 100,
max_depth = 30,
nbins_cats = 1115, ## allow it to fit store ID
training_frame=trainHex)
summary(rfHex)
cat("Predicting Sales\n")
## Load test data into cluster from R
testHex<-as.h2o(test)
## Get predictions out; predicts in H2O, as.data.frame gets them into R
predictions<-as.data.frame(h2o.predict(rfHex,testHex))
## Return the predictions to the original scale of the Sales data
pred <- expm1(predictions[,1])
summary(pred)
train.head()
head(train)
head(test)
train <- fread("train.csv",stringsAsFactors = T)
test  <- fread("test.csv",stringsAsFactors = T)
train$Date = as.Date(train$Date)
train$date[1]
train$date[2]
train$date
as.Date(train$Date)
train$Date = as.Date(train$Date)
train$date
as.Date(train$Date)
train$Date
train$Date[1]
int(train$Date[1])
max(train$Date[1])
train$Date[1] > "2016-10-21"
min(train$Date)
split_train = train[train$Date < "2015-05-31"]
head(split_train)
1<=1
split_train = train[train$Date <= "2015-05-31"]
split_test = train[train$Date >= "2015-06-20"]
len(split_test)
length(split_test)
dim(split_test)
dim(split_train)
dim(test)
dim(pred)
## Return the predictions to the original scale of the Sales data
pred <- expm1(predictions[,1])
dim(pred)
pred
dim(test)
train = split_train
test = split_test
train <- train[Sales > 0,]  ## We are not judged on 0 sales records in test set
## See Scripts discussion from 10/8 for more explanation.
train <- merge(train,store,by="Store")
test <- merge(test,store,by="Store")
cat("train data column names and details\n")
summary(train)
cat("test data column names and details\n")
summary(test)
## more care should be taken to ensure the dates of test can be projected from train
## decision trees do not project well, so you will want to have some strategy here, if using the dates
train[,Date:=as.Date(Date)]
test[,Date:=as.Date(Date)]
# seperating out the elements of the date column for the train set
train[,month:=as.integer(format(Date, "%m"))]
train[,year:=as.integer(format(Date, "%y"))]
train[,Store:=as.factor(as.numeric(Store))]
test[,month:=as.integer(format(Date, "%m"))]
test[,year:=as.integer(format(Date, "%y"))]
test[,Store:=as.factor(as.numeric(Store))]
## log transformation to not be as sensitive to high sales
## decent rule of thumb:
##     if the data spans an order of magnitude, consider a log transform
train[,logSales:=log1p(Sales)]
## Use H2O's random forest
## Start cluster with all available threads
h2o.init(nthreads=-1,max_mem_size='10G')
## Load data into cluster from R
trainHex<-as.h2o(train)
## Set up variable to use all features other than those specified here
features<-colnames(train)[!(colnames(train) %in% c("Id","Date","Sales","logSales","Customers"))]
## Train a random forest using all default parameters
rfHex <- h2o.randomForest(x=features,
y="logSales",
ntrees = 100,
max_depth = 30,
nbins_cats = 1115, ## allow it to fit store ID
training_frame=trainHex)
summary(rfHex)
cat("Predicting Sales\n")
## Load test data into cluster from R
testHex<-as.h2o(test)
## Get predictions out; predicts in H2O, as.data.frame gets them into R
predictions<-as.data.frame(h2o.predict(rfHex,testHex))
## Return the predictions to the original scale of the Sales data
pred <- expm1(predictions[,1])
####
install.packages('MLmetrics')
library(MLmetrics)
RMSPE(pred ,test$Sales )
pred
test$Sales
pred
test$Sales
RMSE(pred ,test$Sales )
P
RMSPE(pred ,test$Sales )
(0-0)/0
(1-0)/0
train
test
head(test)
pred2 = pred
for (i in test$Open) {
j = 1
if (i==0) {
pred2[j]=0
}
j= j+1
}
pred2
print(i)
test$Open
len(test$Open)
length(test$Open)
test$Open[1]
for (i in 1:length(test$Open)) {
if (test$Open[i]==0) {
print(i)
}
}
for (i in 1:length(test$Open)) {
if (test$Open[i]==0) {
pred2[i]=0
}
}
pred2
RMSPE(pred2 ,test$Sales )
rmspe = 0
for (i in 1:length(test$Sales)) {
if (test$Sales[i]!=0) {
rmspe = rmspe + ((test$Sales[i]-pred[i])/test$Sales[i])^2
}
}
rmspe
test[Sales!=0]
rmspe/length(test[Sales!=0])
sqrt(rmspe/length(test[Sales!=0]))
rmspe
length(test[Sales])
length(test$Sales)
sqrt(rmspe/length(test[Sales!=0]$Sales))
